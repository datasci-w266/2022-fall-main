{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 4: Image Captioning\n",
        "\n",
        "This assignment is somewhat short.  We want you to spend your time on the project instead!\n",
        "\n",
        "This assignment explores models connecting different modalities - exploring a connection between images and text.  By the time you're done with this assignment, you'll have:\n",
        "\n",
        "* explored MS COCO captioning dataset\n",
        "* investigated a few captioning techniques\n",
        "* worked with CLIP embeddings for images and captions\n",
        "\n",
        "\n",
        "### Data\n",
        "\n",
        "* Download the 2014 validation images, as well as their annotations from https://cocodataset.org/#download.  On your GCP instance, a command like ```curl http://images.cocodataset.org/zips/val2014.zip --output val2014.zip``` for each will do what you need. You'll also want the file named `annotations_trainval2014.zip`.\n",
        "* Unzip both files and name the corresponding directories `val2014` and `annotations` in this assignment directory.\n",
        "\n",
        "### Explore the dataset\n",
        "Look in the annotations directory.  Which file(s) contain the image captions?  Load those into memory here.  Your end goal is to generate a list of tuples for each of train2014 (you don't need the actual images to do this!) and val2014 i.e. ```[(391895, 'val2014/COCO_val2014_000000391895.jpg', 'A man with a red helmet on a small moped on a dirt road. '), ...]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "import json\n",
        "\n",
        "train2014 = []\n",
        "val2014 = []\n",
        "    \n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert (479495,\n",
        "  'train2014/COCO_train2014_000000479495.jpg',\n",
        "  'A bicycle is parked by a bench at night.') in train2014\n",
        "assert (203564,\n",
        "  'val2014/COCO_val2014_000000203564.jpg',\n",
        "  'A black metal bicycle with a clock inside the front wheel.') in val2014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell if you want to write code to answer the questions below.\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions (Part A)\n",
        "\n",
        "1. How many images do we have captions for in the training set?\n",
        "2. How many captions do we have per image (rounded to nearest integer)?\n",
        "3. If you just split on whitespace and do nothing else, how many words are in the vocabulary?\n",
        "\n",
        "Given your answer to 3, think about what you might need to do with that few examples and that large of a vocabulary?  (Use pretrained embeddings from a massive semi-supervised dataset, keeping only the top-k tokens, smarter tokenization, ...).  You don't need to write your answer anywhere, but given how often these problems arise in NLP, you should be feeling more confident at this point in the course how to handle these situations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's look at some of the examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import random\n",
        "\n",
        "samples = 5\n",
        "n = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for i in range(samples):\n",
        "    image_caption = val2014[random.randint(0, len(val2014))]\n",
        "    image_load = load_img(image_caption[1])\n",
        "    \n",
        "    ax = fig.add_subplot(samples,2,n,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    n += 1\n",
        "    \n",
        "    ax = fig.add_subplot(samples,2,n)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.text(0, 0, image_caption[2], fontsize=20)\n",
        "    n += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundational image captioning papers\n",
        "\n",
        "## Show & Tell\n",
        "\n",
        "[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf) was the first step towards neural image captioning.  Fundamentally it is an encoder-decoder scheme similar to what we've seen in class.  Concretely, it uses the CNN structure of an (at the time) state of the art image classification CNN as the encoder and it uses an LSTM as a decoder.  As in the generation models in class, it continues to generate text until a special \"stop\" token is emitted.  After **reading** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part B)\n",
        "\n",
        "1.  What parts of the CNN were trained?\n",
        "2.  What was the biggest concern when deciding?\n",
        "3.  How was the encoded image representation input into the decoder?\n",
        "4.  Given we are \"translating\" from an image to a caption (without a length constraint), which evaluation metric did the authors determine was reasonable for a top line metric?\n",
        "5.  What beam width is equivalent to one where you select the highest probability word in each decoding step?\n",
        "\n",
        "\n",
        "## Deep Visual Alignment\n",
        "\n",
        "[Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf) is a fun read for which we will ask no questions.  Its critical insights are around understanding an image as a composition of regions, and building upon that understanding to construct both a caption for the whole image, but labels for its consistuent parts.\n",
        "\n",
        "## Show, Attend & Tell\n",
        "\n",
        "[Show, Attend & Tell](https://arxiv.org/pdf/1502.03044.pdf) applies the same \"provide the decoder more context, as directly as possible\" trick we've seen over the course: adding attention.  After **skimming** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part C)\n",
        "\n",
        "1. What is the attention over?\n",
        "2. What do the figures with highlight shading represent in Figures 2, 3 and 5?\n",
        "\n",
        "# Exploring a MS COCO captioner\n",
        "\n",
        "There are many examples of image captioners ML engineers have built on the MS COCO dataset you explored. [This one](https://replicate.com/rmokady/clip_prefix_caption) uses a (more) modern large language model as its decoder, GPT-2.  \n",
        "\n",
        "* **Explore** the samples and play with using beam search and not.  What do you notice?\n",
        "\n",
        "This is an example from the Show & Tell paper of a low-quality caption (see figure 5).  The GPT-2 model proposes \"the car that person drove to the hospital.\" vs. \"A yellow school bus parked in a parking lot\" from the original paper. ![Misclassified](littlecar.png) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP Embeddings and Image Classification\n",
        "\n",
        "The [CLIP paper](https://arxiv.org/pdf/2103.00020.pdf)  describes a system that emits encodings that represent both images and text captions. The system learns to match a picture with its caption so the encoding for the image and the encoding for an associated caption should have a very high cosine similarity.  Systems like DALL-E use CLIP embeddings to generate images based on a text description by using the text encoding to get the image encoding and then processing the image encoding to generate the final image.  We're going to use CLIP in the opposite direction.  Namely we're going to use CLIP embeddings to classify images, that is to score a set of captions for an image based on the image's content.\n",
        "\n",
        "We can use the HuggingFace implementation of CLIP to experiment with this multimodal capability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, TFCLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's begin our experiment.  We're going select two images that contain both zebras and cars.  They may contain other things as well.  We're also going to generate a set of captions that we will score.  Specifically, we'll pass the output for the captions through a softmax to give us a probability distribution over the four captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example tags: animal = zebra, transport = car\n",
        "\n",
        "urls = [\"https://farm1.staticflickr.com/9/15631288_605abb3096_z.jpg\", #zebras foreground, cars background\n",
        "        \"https://farm4.staticflickr.com/3057/3033996041_11293469b7_z.jpg\"]  #zebra foreground, tiny car background\n",
        "captions = [\"a photo of cars\",\n",
        "            \"a photo of a giraffe\",\n",
        "            \"a photo of zebras in a field\",\n",
        "            \"a photo of some zebras and cars\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "    \n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CLIP embeddings allow us to associate captions with images.  Specifically, we can build a classifier that assigns probabilities to each of the captions.  We want the highest probability to go to the most descriptive caption out of the four captions for the given image.  Notice here that even though both images contain zebras, one of them features a line of clearly visible cars.  The other image only has one small car off in the distance.  Note that the first image with the cars scores high for the caption of ```a photo of some zebras and cars``` because the zebras and cars are very visible.  The second image scores highest for ```a photo of zebras in a field``` but the small car is less noticed but scores above a zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example tags: two dogs in bike, human bike tiny dog\n",
        "\n",
        "urls = [\"http://farm1.staticflickr.com/8/10896131_6a184b48cb_z.jpg\",  #2 dogs in bike basket\n",
        "        \"http://farm4.staticflickr.com/3082/2797293301_dd26fd613f_z.jpg\"] #human and bike with tiny dog\n",
        "captions = [\"a photo of a dog\",\n",
        "            \"a photo of some dogs in a basket\",\n",
        "            \"a photo of a bike\",\n",
        "            \"a photo of some dogs with a bike\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "    \n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, these two images both contain bicycles and dogs.  The first image is two dogs in a basket on the front of a bike.  While the bike is visible, the two dogs are the focus of the image.  The second image features a person with their bike.  The bike happens to contain a small dog.  We would expect the embeddings to reflect the different emphases of the photos and indeed they do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example tags: animal = dog, transport = bike\n",
        "\n",
        "urls = [\"http://farm1.staticflickr.com/124/405495389_d4316b1224_z.jpg\",   #dog foreground and tiny bikes background\n",
        "        \"http://farm8.staticflickr.com/7194/6991675037_3c298541c0_z.jpg\"] #motorbike foreground, many bikes and tiny dog background\n",
        "captions = [\"a photo of a dog\",\n",
        "            \"a photo of a motorbike\",\n",
        "            \"a photo of a plane\",\n",
        "            \"a photo of some bikes\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "    \n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the third example, the first image includes a dog in the foreground and a number of small bikes in the distant background.  You can look at the annotations associated with the image to see where these objects are located. The second image includes a motorbike/motorcycle in the the foreground but a number of bikes and a tiny dog in the background.  Again we're hand crafting these captions to include the items in the image but we want the score for the caption to reflect what's in the foreground of the image.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it is your turn.  You will essentially replicate the examples above but you will do it with images **you** select.  First you need to select *two* images for processing. Go to [the COCO Explorer](https://cocodataset.org/#explore), click on two tag icons: an animal (see icon column of animals) and a mode of transportation (see icon column of ), and search. (You pick which; you might have to try a few combinations until you get multiple image results.)\n",
        "\n",
        "Find two different images that each contain your animal and your mode of transportation.  It's okay if they contain other things as well.  If you click on the URL icon above each image, you'll see a link to the annotated image and the original (unlabeled) image. Put the original image link in the code cell below *your image 1 url* and *your image 2 url*, then create four captions that mention only one of the objects each vs both objects together. You can see the captions we created for the three examples above.  The goal is to get probabilities above 0.85 for the caption that best describes the first image and the caption that best describes the second image.\n",
        "\n",
        "As in the examples above, you must find a pair of images with the same two objects tagged in them, but which get different results for which caption has the highest probability according to the CLIP model.\n",
        "\n",
        "Note which object tags you used, and give a brief explanation of what looks different about the two images that you think made them get different CLIP results for the most likely caption.  Enter that explanation in the cell below.  You **do not need to enter it in the answers sheet**.  Just leave it in the notebook that you submit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example tags: animal = ???, transportation = ???\n",
        "\n",
        "urls = [\"your image 1 url\",   #\n",
        "        \"your image 2 url\"] #\n",
        "captions = [\"caption 1\",\n",
        "            \"caption 2\",\n",
        "            \"caption 3\",\n",
        "            \"caption 4\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "    \n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions (Part D)\n",
        "\n",
        "1. What is the animal tag you selected?\n",
        "\n",
        "2. What is the transportation tag you selected?\n",
        "\n",
        "3. What is the probability associated with the most likely caption for image 1?\n",
        "\n",
        "4. What is the probability associated with the most likely caption for image 2?\n",
        "\n",
        "**(Answer 5 below but do NOT enter your sentences in the answers file)**\n",
        "\n",
        "5. Why do you think the differences between your two images are reflected in the 4 captions you produced.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please answer in two to four sentences right here: \n",
        "\n",
        "*YOUR Q 5 ANSWER HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Yay, you're done with your W266 homework.  Now focus on your project!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
