{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning in TensorFlow and Keras\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the training set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
        "\n",
        "$$y_{hat} = xW + b$$\n",
        "\n",
        "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: \n",
        "\n",
        "$$y_{hat} = \\sigma(xW + b)$$\n",
        "\n",
        "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../a1/information_theory.ipynb) between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "### Preliminaries...\n",
        "\n",
        "Before we do anything else, let's load our data and take a quick look at it.  In this example, we're going to build a (very) simple binary classifier based on two floating point features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import data\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = data.generate_data(2500, 500) #large test size to make diagrams better\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='bwr');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Logistic Regression\n",
        "\n",
        "It's clear that the data is separable with a vertical line.  The simplest model we can use for this data is logistic regression.  Let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#####\n",
        "## MAKE SURE YOU UNDERSTAND THIS CODE!!\n",
        "##\n",
        "## Look up keras.Sequential and keras.layers.Dense!\n",
        "##\n",
        "## You will need to use them to write your own model down below!\n",
        "#####\n",
        "\n",
        "# Sequential models are ones where the set of specified layers are stacked each on top of the previous.\n",
        "linear_model = keras.Sequential([\n",
        "    # Dense is an affine (xW + b) layer followed by an element wise nonlinearity.\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# adam optimizer is a fancier version of gradient descent.  You can read more about it here: https://arxiv.org/pdf/1412.6980.pdf\n",
        "linear_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # From information theory notebooks.\n",
        "              metrics=['accuracy'])        # What metric to output as we train.\n",
        "\n",
        "linear_model.fit(X_train, y_train, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hint:** You should expect to see an initial loss here of 0.2 - 1.2.  This is because a well-initialized random classifier tends to output a uniform distribution.  For each example in the batch, we either compute the cross-entropy loss of the label (`[1, 0]` or `[0, 1]`) against the model's output (`~[0.5, 0.5]`).  Both cases result in $-\\lg(0.5) = lg(2) = 1.0$.\n",
        "\n",
        "Of course, your random classifier won't output exactly uniform distributions (it's random after all), but you should anticipate it being pretty close.  If it's not, your initialization may be broken and make it hard for your network to learn.\n",
        "\n",
        "**[Optional]** Some technical details... if your randomly initialized network is outputting very confident predictions, the loss computed may be very large while at the same time the sigmoids in the network are likely in saturation, quickly shrinking gradients.  The result is that you make tiny updates in the face of a huge loss.\n",
        "\n",
        "Let's use our model to make predictions on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = linear_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### But what about a harder problem?\n",
        "\n",
        "The case above, the data was linearly separable making it susceptible to a linear classifier.\n",
        "\n",
        "But what if you had data that looked more like this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = data.generate_non_linear_data(2500, 500)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap='bwr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, let's make predictions on the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = linear_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That isn't very good!\n",
        "\n",
        "### Building a deeper network\n",
        "\n",
        "Ok, now it's your turn.  Build a deeper neural network below.  Try to achieve a loss less than 0.05.  Initialization is random, but try to make it happen \"almost\" always (e.g. 90% of the time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "deep_model = keras.Sequential([\n",
        "    # Hint, try \"relu\" as your activation function.\n",
        "    # relu(z) = max(0, z).\n",
        "    #     Note that: relu(z) = z when z > 0\n",
        "    #                relu(z) = 0 otherwise\n",
        "    #\n",
        "    # See https://en.wikipedia.org/wiki/File:Ramp_function.svg\n",
        "    #\n",
        "    # This is the most common nonlinearity for the main body of the network as its derivative is\n",
        "    # either 0 or 1, depending on the value of z.\n",
        "    #\n",
        "    # This means that the gradient doesn't tend to explode or vanish as you multiply more partial\n",
        "    # derivative terms together.\n",
        "    #\n",
        "    # For this problem...\n",
        "    #\n",
        "    # Try toying with the trade offs between more layers vs wider networks:\n",
        "    #   If we keep repeating the same hidden layer with n neurons: \n",
        "    #    What's the minimum number of hidden layers you can get away with given a larger value of n?\n",
        "    #    Conversely, what's the smallest number of neurons (n) in each layer you can use \n",
        "    #     if you use more hidden layers?\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    # Think about why you still use a sigmoid at the top of your network.\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "deep_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "deep_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predictions = deep_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations...\n",
        "... you've trained a nonlinear classifier with TensorFlow and Keras!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
