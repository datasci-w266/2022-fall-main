{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b4f3da",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "This notebook provides an overview of back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e6ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122445f5",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "## Chain Rule\n",
    "\n",
    "We'll see backpropagation is just chain rule, so let's nail that down before we get into backprop.\n",
    "\n",
    "Two notations and one picture...\n",
    "\n",
    "### Functions\n",
    "\n",
    "If $h(x) = f(g(x))$\n",
    "\n",
    "then $h^{'} = f^{'}(g(x))g^{'}(x)$\n",
    "\n",
    "### Using intermediate variables\n",
    "\n",
    "$z = f(g(x))$\n",
    "\n",
    "Let $y=g(x)$.\n",
    "\n",
    "Then $z = f(y)$.\n",
    "\n",
    "$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}$\n",
    "\n",
    "### In pictures\n",
    "\n",
    "If moving the left-most rod some small amount moves the middle rod by 2x that amount and moving the middle rod by some small amount moves the right-most rod by 3x that amount, then moving the left-most rod a small amount moves the right one by 6x that amount.\n",
    "\n",
    "![in pictures](derivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e0bc3",
   "metadata": {},
   "source": [
    "## Computing a derivative\n",
    "\n",
    "The definition of a derivative is $f^{'}(x) = lim_{h->0}\\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "In a picture, this formula looks like this, as the two points move closer to one another.\n",
    "\n",
    "![tangent](tangent.png)\n",
    "\n",
    "There are two ways of computing the gradient: analyically and numerically.\n",
    "\n",
    "### Numerically\n",
    "\n",
    "The naive approach to computing the gradient is to take the formula / picture and perform the math at some point on some function.  To do this, you execute the function twice, once at $f(x)$ and again at $f(x+h)$ for a small h.  Since you can't let $h$ go to zero, often we use a point slightly on either side of the point of interest $f^{'}(x) = lim_{h->0}\\frac{f(x+h) - f(x-h)}{2h}$.\n",
    "\n",
    "For example, to compute the derivative of $f(x) = x^2$ at $x=3$, we'd pick a small h (say, 0.000001) and find the slope of the line between the points (3-h, f(3-h)) and (3+h, f(3+h)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8520fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000000000838668"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_derivative(func, x, h):\n",
    "    # Finds the derivative of func at x.\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "f = lambda x: x * x\n",
    "numerical_derivative(f, 3, 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afbe4c",
   "metadata": {},
   "source": [
    "### Analytically\n",
    "\n",
    "This is way you learned in high school calculus.  Using the above definition, you can compute the gradient of any function.  For example, $f(x) = x^2$, you'd have:\n",
    "\n",
    "$$\n",
    "f^{'}(x) = lim_{h->0}\\frac{f(x+h) - f(x)}{h}\n",
    " = lim_{h->0}\\frac{(x+h)^2 - f(x)^2}{h}\n",
    " = lim_{h->0}\\frac{2xh + h^2}{h}\n",
    " = lim_{h->0}2x + h\n",
    " = 2x\n",
    "$$\n",
    "\n",
    "Evaluating this at $x=3$, you'll see the numerical solution is very close to the analytic one.\n",
    "\n",
    "**Now you have a rule** for any $x$.\n",
    "\n",
    "After a few of these, you start to notice some patterns and derive general rules, i.e. if $f(x) = x^n$, then $f^{'}(x) = nx^{n-1}$.\n",
    "\n",
    "**Now you have a rule for all functions of a particular form!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d607d",
   "metadata": {},
   "source": [
    "# An Example: Linear Regression\n",
    "\n",
    "In a linear regression model, you have a feature vector $x$ and a weight vector $w$ that, together, produce a real valued output $\\hat{y}$.  During training, this prediction is matched against the desired/labeled output, $y$ and squared error is used to quantify the loss, $L = (y - \\hat{y})^2$.\n",
    "\n",
    "![linear regression](linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c13fc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat = xw = 12.0\n",
      "loss = (y_hat - y)^2 = 9.0\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Model with a single feature (and therefore a single weight).\n",
    "w = np.array([4.0])\n",
    "\n",
    "# In some specific training example, x = 3 and labeled with y = 15.\n",
    "x = np.array([3.0])\n",
    "y = 15\n",
    "\n",
    "# Run the model\n",
    "y_hat = x.dot(w)\n",
    "print('y_hat = xw = {}'.format(y_hat))\n",
    "\n",
    "# Compute the loss.\n",
    "loss = np.power(y_hat - y, 2)\n",
    "print('loss = (y_hat - y)^2 = {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189ca09",
   "metadata": {},
   "source": [
    "### How does our network learn from this example? \n",
    "\n",
    "Conceptually, the question we ask is this: \"if we tweak the weight a bit one way, does the loss get better or worse?  what about the other way?\".  We then move the loss in the \"downhill\" direction.  We call this algorithm gradient descent and has several implications:\n",
    "\n",
    "* If neither direction makes it better, we should just leave it alone.\n",
    "* We only have local, first order, information.  Maybe tweaking it makes it worse in some direction, but if you move it quite a bit farther away then there's an operating point that's a lot better?\n",
    "* What if some weight isn't involved in making many predictions vs. some that are involved in every prediction... should the size of tweak be the same for both, or do you want to be more aggressive when you know you'll only get a few changes to make changes?  (This most often arises in the weights associated with the embeddings of rare words.)\n",
    "\n",
    "Various optimizer strategies (momentum, adagrad, adam) have been developed to address these implications.  For the remainder of this notebook, we'll stick to the basic gradient descent formulation of the above intuition:\n",
    "\n",
    "$\\theta := \\theta - \\alpha \\frac{dL}{d\\theta}$ where $\\theta$ is a vector of all the tunable weights in our model and alpha is a scaling parameter that determines how much we tweak parameters before computing a new gradient estimate.\n",
    "\n",
    "Let's apply this to the single training example pushed through the network above.  In our example the trainable parameters, $\\theta$, consists only of a single weight, $w$.  We need to compute $\\frac{dL}{dw}$.\n",
    "\n",
    "$\n",
    "\\frac{dL}{dw}\n",
    "= \\frac{dL}{d\\hat{y}} \\times \\frac{d\\hat{y}}{dw}\n",
    "= (2\\hat{y} - 2y) \\times x\n",
    "= (2 \\times 12 - 2 \\times 15) \\times 3\n",
    "= -18\n",
    "$\n",
    "\n",
    "Let's check our analytical work with the numerical approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e69c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical gradient: -18.000001489326678\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_with_weight(w):\n",
    "    x = np.array([3.0])\n",
    "    y = 15\n",
    "    y_hat = x.dot(w)\n",
    "    loss = np.power(y_hat - y, 2)\n",
    "    return loss\n",
    "\n",
    "h = 0.0000000001\n",
    "w = np.array([4.0])\n",
    "numerical_grad = (compute_loss_with_weight(w + h) - compute_loss_with_weight(w - h)) / (2 * h)\n",
    "print('Numerical gradient: {}'.format(numerical_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc146982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2b0e121f0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjC0lEQVR4nO3dd3hUdd738fc3k0oaJQkJXXqRECAUUVEstwgoemNdRayoa1236JZn73V3H2/X3kVQEAtWdFVWF1kLoCgQIASQjvQIQXoLKb/njww+EYkMyUzOTPJ5XddcM3PmDPPxXGc/e3Laz5xziIhI5InyOoCIiFSPClxEJEKpwEVEIpQKXEQkQqnARUQiVHRt/lhaWppr06ZNbf6kiEjEmzdv3jbnXPqR02u1wNu0aUNeXl5t/qSISMQzs3VHm65dKCIiEUoFLiISoVTgIiIRSgUuIhKhVOAiIhFKBS4iEqFU4CIiESoiCnzB+h2Mmb7a6xgiImGlVi/kqa5/LtjExK/WkZkSzwU9m3sdR0QkLETEFvifhnWlf9vG/G5yAQvW7/A6johIWIiIAo/xRfHsFb3JTIln9Mvz2LzzgNeRREQ8d8wCN7N4M5tjZgvNbImZ3euf/hcz22Rm+f7HkFAGbZQYywujcjlwqIwbXspj/6HSUP6ciEjYC2QLvBg4wznXA8gBBptZf/9njzrncvyPD0MV8rAOTZN58vKefFO4m9+8tZDyco3nKSL11zEL3FXY638b43941pyDOmfwh3O78OGi73j8k5VexRAR8VxA+8DNzGdm+cBWYJpzbrb/o1vNrMDMxptZoyq+O9rM8swsr6ioKCihrz/1BC7u3YLHP1nJlILNQfk3RUQiTUAF7pwrc87lAC2AvmZ2IvAs0I6K3SqFwMNVfHescy7XOZebnv6T+5FXi5nx9wtPJLd1I37z1kIWbdwVlH9XRCSSHNdZKM65ncDnwGDn3BZ/sZcD44C+wY9XtbhoH2NG9qZJYhw3vJTH1t0Ha/PnRUQ8F8hZKOlm1tD/OgE4C1hmZlmVZrsQWByShD8jLSmOcVflsvtgCTe8PI+DJWW1HUFExDOBbIFnAZ+ZWQEwl4p94FOAB8xskX/6IOBXIcxZpa7NUnj00hwWbtjJ3ZMLcE5npohI/XDMS+mdcwVAz6NMHxmSRNVwTrdMfntOJx6cupyOTZO5ZVB7ryOJiIRcRNwLJRC/PL0dK7bs4cGpy+mQkcR/dcv0OpKISEhFxKX0gTAz/jEimx4tG3LnG/l8s3m315FEREKqzhQ4QHyMj3Eje5MSH8MNL+WxbW+x15FEREKmThU4QEZKPOOuyuX7fcXc9PI8ikt1ZoqI1E11rsABurdI5eGLc8hbt4M/vbtYZ6aISJ1UJwscYGh2Fnec2YG35m3k+Znfeh1HRCTo6sxZKEdzx5kdWLl1D/d9tJT2GUkM6pzhdSQRkaCps1vgAFFRxkMX96BrVgq3vbZAZ6aISJ1SpwscoEFsNC+M6kNyfDTXvjiXwl0azUdE6oY6X+AAmanxTLimD/uKS7lmwlx2HyzxOpKISI3ViwIH6JyZwrNX9mbV1r388pX5lJSVex1JRKRG6k2BA5zSIY3//e/ufLFqG79/Z5FOLxSRiFanz0I5motzW7Jp5wEe+89KWjRK4M6zOnodSUSkWupdgUPF6YUbdxwu8QZc1LuF15FERI5bvSxwM+O+C7vz3a6D3DO5gMyUeE7pkOZ1LBGR41Kv9oFXFhsdxTNX9qJ9RhI3vTKPpYU6R1xEIku9LXCAlPgYJlzTh8Q4H9dMmMt3uzSupohEjnpd4ABZqQlMuLove4tLuebFuezROeIiEiECGdQ43szmmNlCM1tiZvf6pzc2s2lmttL/3Cj0cUOja7MUnrmiFyu27OGXr+occRGJDIFsgRcDZzjnegA5wGAz6w/cA3zinOsAfOJ/H7EGdkznfy/szsyV2/jjuzpHXETC3zEL3FXY638b4384YDgw0T99InBBKALWpkv6tOT2M9rzZt5Gnvx0lddxRER+VkD7wM3MZ2b5wFZgmnNuNtDUOVcI4H8+6r1azWy0meWZWV5RUVGQYofOr87uyH/3bM4j01Ywed5Gr+OIiFQpoAJ3zpU553KAFkBfMzsx0B9wzo11zuU653LT09OrGbP2mBn3j8hmQLsm3D25gC9XbfM6kojIUR3XWSjOuZ3A58BgYIuZZQH4n7cGO5xXYqOjePbK3rRNT+Sml+ex/Ls9XkcSEfmJQM5CSTezhv7XCcBZwDLgfWCUf7ZRwHshyuiJ1IQYJlzTl4RYH9dMmMOW3TpHXETCSyBb4FnAZ2ZWAMylYh/4FOB+4GwzWwmc7X9fpzRvmMD4q/uw60AJV+s+4iISZqw2T5fLzc11eXl5tfZ7wTJ9RRHXvTiXXq0b8dK1fYmP8XkdSUTqETOb55zLPXJ6vb8SMxCndUzn4Ut6MHftdm6dpAt9RCQ8qMADNDynOX89vxv/WbqVu98uoLxcF/qIiLfq5e1kq2vkSW3Ysb+ER6atoGGDWP7PsC6YmdexRKSeUoEfp9vOaM/2fYcY/+W3NE6M4dYzOngdSUTqKRX4cTIz/jysK7sOlPDQxytIbRDLyP6tvY4lIvWQCrwaoqKMBy7KZveBEv783mIaJsRwXo9mXscSkXpGBzGrKcYXxdNX9KJP68bc9WY+01eE/31eRKRuUYHXQHyMj3GjcmmfkcxNL89j3rodXkcSkXpEBV5DqQkxvHRtX5qmxHHti3N13xQRqTUq8CBIT47j5ev6ER8TxcgXZrNh+36vI4lIPaACD5KWjRvw0rX9KC4t58oXZrN1j25+JSKhpQIPok6ZyUy4pg9bdxczavxcdh3Qza9EJHRU4EHWq1UjnhvZm1Vb93D9xLkcOFTmdSQRqaNU4CEwsGM6j16aQ966Hdyim1+JSIiowENkWHYz/n7BiXy6bCu/fWuhbn4lIkGnKzFD6Ip+rdm5v4QHpy6nYYNY/ue8rrr5lYgEjQo8xH55ejt27DvE8198S1xMFPcM7qwSF5GgUIGHmJnxx6FdOFhaxnPT1xATFcWv/6ujSlxEakwFXgvMjL+efyKlZY6nPltFtM+486yOXscSkQgXyKj0Lc3sMzNbamZLzOwO//S/mNkmM8v3P4aEPm7kiooy7ruwOxf1bsFj/1nJ05+t8jqSiES4QLbAS4FfO+fmm1kyMM/Mpvk/e9Q591Do4tUtUVHGP0ZkU1bueHDqcmJ8xuiB7byOJSIR6pgF7pwrBAr9r/eY2VKgeaiD1VW+KOPBi7IpLXfc9+EyfFFRXHfKCV7HEpEIdFzngZtZG6AnMNs/6VYzKzCz8WbWqIrvjDazPDPLKyrSPbMBon1RPHJJD849MZO/TfmGl75a63UkEYlAARe4mSUBk4E7nXO7gWeBdkAOFVvoDx/te865sc65XOdcbnp6es0T1xExvigev6wnZ3Vpyp/fW8Kk2eu9jiQiESagAjezGCrK+1Xn3DsAzrktzrky51w5MA7oG7qYdVNsdBRPX9GTQZ3S+cO7i3hz7gavI4lIBAnkLBQDXgCWOuceqTQ9q9JsFwKLgx+v7ouL9vHslb05tUMad79TwOR5G72OJCIRIpCzUE4GRgKLzCzfP+0PwOVmlgM4YC1wYwjy1QvxMT7GXZXLdRPn8tu3FxLtM4bn6DixiPy8QM5C+QI42mWDHwY/Tv0VH+Pj+av6cPWEOdz15kKio6IYmp117C+KSL2luxGGkYRYH+Ov7kOvVg254/UFTF3yndeRRCSMqcDDTGJcNBOu6Uv3FqncOmk+nyzd4nUkEQlTKvAwlBQXzcRr+9IlK4WbX5nP58u3eh1JRMKQCjxMpcTH8PK1/ejQNInRL89j5kpdBCUiP6YCD2OpDWJ45bp+tE1L5PqJedoSF5EfUYGHuUaJsUy6oT/t0pO44aU8HdgUkR+owCNA48RYXruhP92apfLLV+fzXv4mryOJSBhQgUeI1AYxvHJ9P3JbN+LON/J12b2IqMAjSVJcNC9e05dT2qfxu8kFTJy11utIIuIhFXiESYj18fyoXM7u2pT/eX8JY6av9jqSiHhEBR6B4qJ9PHNFL87r0Yz7P1rGo9NW4JzzOpaI1DINahyhYnxRPHZpDvHRUTz+yUoOlJTx+3M7a7R7kXpEBR7BfP4xNhNifYydsYYDh8q49/xuREWpxEXqAxV4hIuKMu49vxsJMT6em7GGAyVl/GNENj6VuEidpwKvA8yMe87tTEKsj8f+s5KDJWU8emkOMT4d4hCpy1TgdYSZcedZHUmI8fG/Hy2juLScp37Rk7hon9fRRCREtIlWx9x4Wjv+Orwb077ZwvUT8zhwqMzrSCISIirwOuiqk9rwwIhsvli1jVET5rC3uNTrSCISAoEMatzSzD4zs6VmtsTM7vBPb2xm08xspf+5UejjSqAu6dOSxy7NYd66HVz5/Gx27S/xOpKIBFkgW+ClwK+dc12A/sAtZtYVuAf4xDnXAfjE/17CyPCc5jxzRS++2byby8d9zdY9B72OJCJBdMwCd84VOufm+1/vAZYCzYHhwET/bBOBC0KUUWrgnG6ZjBuVy7fb9jHi2Vl8u22f15FEJEiOax+4mbUBegKzgabOuUKoKHkgo4rvjDazPDPLKyrSqDJeOK1jOq+N7s++4jJGPDuL/A07vY4kIkEQcIGbWRIwGbjTObc70O8558Y653Kdc7np6enVyShBkNOyIZNvHkBinI/Lx37NZxrdRyTiBVTgZhZDRXm/6px7xz95i5ll+T/PAtQIYe6EtEQm3zyAtukVQ7S9lad7iotEskDOQjHgBWCpc+6RSh+9D4zyvx4FvBf8eBJsGcnxvHHjSZzUtgm/fbuApz9bpTsZikSoQLbATwZGAmeYWb7/MQS4HzjbzFYCZ/vfSwRIiotm/NV9GJ7TjAenLud/3l9CWblKXCTSHPNSeufcF0BVd0Y6M7hxpLbERkfx6CU5NE2JZ+yMNRTtKebRS3OIj9Gl9yKRQvdCqceioow/DOlCRnIcf//XUr7fN4dxV+WSmhDjdTQRCYAupReuP7UtT1zekwXrd3DJmK8o3HXA60giEgAVuABwfo9mvHhNXzbtPMCIZ2axcsseryOJyDGowOUHJ7dP440b+1NS7rhozFfkrd3udSQR+RkqcPmRbs1SeefmATRJjOWK52czdcl3XkcSkSqowOUnWjZuwNs3D6BLVgo3vzKPV2ev8zqSiByFClyOqnFiLJNu6MfpnTL447uLeeTj5brgRyTMqMClSg1ioxk7sjeX5LbgiU9Xcfvr+Rws0Qg/IuFC54HLz4r2RfGPEdmckJbEA1OXsX77fsaN7E1GSrzX0UTqPW2ByzGZGTef3o4xV/Zm5ZY9DH/6SxZv2uV1LJF6TwUuATunWyZv3XQSBlw85iv+vbjQ60gi9ZoKXI5Lt2ap/PPWk+mUmcxNr8znqU9X6uCmiEdU4HLcMpLjeX10fy7IacZDH6/gV2/o4KaIF3QQU6olPsbHo5fm0KFpMg9OXc667ft5bmRvMpJ1cFOktmgLXKrNzLhlUHvGXNmLZYV7uOCpL/lmc8Cj7YlIDanApcYGn5jFWzedhAMuGjNLl9+L1BIVuATFic1Tee+Wk+nQNJmbXpnHM59rqDaRUFOBS9BkpMTzxuj+DMtuxgP/Xs6v31xIcakOboqEig5iSlDFx/h44rIcOmQk8ci0FT8c3ExLivM6mkidE8io9OPNbKuZLa407S9mtumIQY5FgIqDm7ef2YFnrujFks27GP7Ulywt1MFNkWALZBfKi8Dgo0x/1DmX4398GNxYUhcM6Z7FWzcOoLS8nBHPzuKDhZu9jiRSpxyzwJ1zMwANzSLV0r1FKu/fegpdslK47bUF3PvBEg6VlnsdS6ROqMlBzFvNrMC/i6VRVTOZ2WgzyzOzvKKiohr8nESqpikVV25ee/IJTPhyLZeP+5rvdh30OpZIxKtugT8LtANygELg4apmdM6Ndc7lOudy09PTq/lzEulifFH8+byuPPWLniwr3M3QJ2Yya9U2r2OJRLRqFbhzbotzrsw5Vw6MA/oGN5bUVcOym/HerSfTKDGWK1+YzTOfr6K8XOeLi1RHtQrczLIqvb0QWFzVvCJHap+RzHu3nMxQ//nio1/OY9f+Eq9jiUScQE4jfA34CuhkZhvN7DrgATNbZGYFwCDgVyHOKXVMYlw0T1yWw1/O68rny4s476kvWLJZg0SIHA+rzcudc3NzXV5eXq39nkSGeet2cMur89mx/xB/u+BELslt6XUkkbBiZvOcc7lHTtel9OK53q0bMeX2U+jduhG/e7uAeyYX6P7iIgFQgUtYSEuK4+Xr+nHroPa8PncDF42ZxYbt+72OJRLWVOASNnxRxm/O6cQLo3JZ//1+hj4xk0+XbfE6lkjYUoFL2DmzS1Om3HYqLRs34NoX83j44+WU6VRDkZ9QgUtYatWkAZNvHsCluS158tNVjBo/h627dfWmSGUqcAlb8TE+/nFRNg+MyCZv3XYGPz6Tad9ol4rIYSpwCXuX9GnJlNtOITMlnhteyuOP7y7iwCGdpSKiApeI0D4jmXdvGcCNA9vy6uz1DH1yJos36cIfqd9U4BIx4qJ9/H5IF169vh/7iku58JkvGTtjte6lIvWWClwizsnt0/j3HQM5s3NT7vtwGVe+MFu3p5V6SQUuEalRYizPXtmLB0Zkk79hJ+c8NoOPFhV6HUukVqnAJWKZGZf0acm/bj+V1k0acPOr87n77QL2FZd6HU2kVqjAJeKdkJbI5JsHcMugdrw5bwNDn5jJwg07vY4lEnIqcKkTYnxR/Paczrx2Q38OlVYMovz0Z6t0BafUaSpwqVP6t23CR3cMZPCJmTw4dTmXj/2ajTt0Uyypm1TgUuekNojhyct78sglPfimcDfnPj6T9/I3eR1LJOhU4FInmRn/3asFH95+Kh0ykrjj9Xxuenme7qcidYoKXOq0Vk0a8OaNJ3H34M58unwrZz0ynTfzNlCbI1GJhEogY2KON7OtZra40rTGZjbNzFb6nxuFNqZI9UX7orj59Hb8+45T6ZyZwu/eLuCq8XM0YIREvEC2wF8EBh8x7R7gE+dcB+AT/3uRsNY2PYnXR/fnb8O7MX/dDv7r0RlM+PJbnakiEeuYBe6cmwFsP2LycGCi//VE4ILgxhIJjagoY+RJbfj4rtPoe0Jj7v3gGy4eM4tVW/d4HU3kuFV3H3hT51whgP85o6oZzWy0meWZWV5RUVE1f04kuJo3TODFa/rwyCU9WLNtH0Me/4KnPl1JSVm519FEAhbyg5jOubHOuVznXG56enqof04kYIfPVJn2q9M4u1tTHvp4Bec9+QWLNuo2tRIZqlvgW8wsC8D/vDV4kURqV3pyHE//ohfPjezN9n2HuOCZL7n/o2UcLNGgERLeqlvg7wOj/K9HAe8FJ46Id87plsm0u07jol4tGDN9Nec+PpPZa773OpZIlQI5jfA14Cugk5ltNLPrgPuBs81sJXC2/71IxEtNiOEfF2Xz6vX9KC0v59KxX/Onfy5iz8ESr6OJ/ITV5gUNubm5Li8vr9Z+T6Qm9h8q5aGpK5gw61syU+L509CuDOmeiZl5HU3qGTOb55zLPXK6rsQUqUKD2Gj+fF5XJt88gIYNYrll0nx+MW42y7/TKYcSHlTgIsfQq1Ujptx2Cn+74ESWfrebIU/M5C/vL2HXfu1WEW+pwEUC4IsyRvZvzWe/Pp3L+7bkpa/WMujhz3ltznpdySmeUYGLHIdGibH8/YLufHDbKbRPT+L37yzigqe/ZN66HV5Hk3pIBS5SDd2apfLGjf15/LIcivYUM+LZWdz1Zr5uVyu1SgUuUk1mxvCc5nzy69P45entmLKwkDMens7YGas5VKpL8iX0VOAiNZQYF83vBnfm418NpN8Jjbnvw2UMfnwG01fo3j8SWipwkSBpk5bIC1f3YcLVfXAORo2fw/UT81j/ve47LqGhAhcJskGdM5h650DuObczX63exlmPTufBqct0NacEnQpcJARio6O46bR2fPqb0xnWPYunP1vNwAc+Y9yMNbpJlgSNLqUXqQWLNu7iwY+XM2NFEZkp8dx+Zgcuzm1BjE/bUHJsupRexEPdW6Ty0rV9eX10f5o1jOcP7y7i7Eem817+Jsp1IZBUkwpcpBb1b9uEyTcP4IVRucTH+Ljj9XyGPDGTT5ZuoTb/Gpa6QQUuUsvMjDO7NOXD20/l8ctyOFBSxnUT87hozFd8rfuPy3FQgYt4JCqq4kKg/9x1Gvdd2J2NO/Zz2divuWr8HA3rJgHRQUyRMHGwpIyXv1rHM5+vYsf+EoZ0z+SuszvRPiPJ62jisaoOYqrARcLMnoMljJv5LS/MXMOBkjJG9GrBHWd1oEWjBl5HE4+owEUizPd7i3nm89W8/PU6nHNc2LM5owe2pX1GstfRpJapwEUi1OadB3j289W8mbeB4tJyzu7alJtOa0fv1o28jia1JCQFbmZrgT1AGVB6tB+oTAUuUn3f7y1m4qy1TPxqHbsOlNC3TWNuOr0tgzplaJzOOi6UBZ7rnNsWyPwqcJGa21dcyutzN/DCzDVs3nWQTk2TGT2wLefnNNOVnXWUClykjikpK+eDhZt5bvoalm/ZQ7PUeK47tS2X9WlJYly01/EkiEJV4N8COwAHPOecG3uUeUYDowFatWrVe926ddX+PRH5Keccny3fypjpa5jz7XZSE2IYdVJrRg1oQ5OkOK/jSRCEqsCbOec2m1kGMA24zTk3o6r5tQUuElrz1+9gzOermbZ0C7G+KC7JbckNp7alVROdghjJQn4Wipn9BdjrnHuoqnlU4CK1Y9XWvYybsYZ3FmykrNwxpHsWV53Uhj5tGumAZwQKeoGbWSIQ5Zzb4389Dfirc+7fVX1HBS5Su7bsPsj4L75l0uz17CkupUNGEpf3bcWIXi1IbRDjdTwJUCgKvC3wrv9tNDDJOfd/f+47KnARb+w/VMqUhYVMmrOe/A07iYuOYmh2Flf0a0WvVtoqD3e6kEdEAFiyeRevzVnPPxdsZm9xKZ2aJvOLfq24oGdzUhO0VR6OVOAi8iP7ikv5YOFmJs1ZT8HGXcTHRHFedjMu79eKni0baqs8jKjARaRKizft4tXZ63k/fxP7DpXROTOZK/xb5cnx2ir3mgpcRI5pb3Ep7+VvYtLs9SzZvJuEGB/n96jYKu/RIlVb5R5RgYtIwJxzFGzcxaTZ63l/4WYOlJTRpkkDhmU3Y1iPLDo1TVaZ1yIVuIhUy+6DJXxYUMiUgkJmrd5GuYP2GUkMy85iWHYzDThRC1TgIlJj2/YW89Hi7/hg4Wbmrt2Oc9AlK4Vh2Vmcl91MV3yGiApcRIJqy+6D/KugkCkFm5m/ficA2S1SGZadxdDsZjRvmOBtwDpEBS4iIbNp5wH+VbCZKQWFFPgHZO7VqiHDspsxNDuLpinxHieMbCpwEakV677fxxT/PvOlhbsxgz6tGzOocwYDO6bRNStFB0CPkwpcRGrd6qK9TFlYyEeLC1n23R4A0pPjOLVDGqd1TOeU9mm65W0AVOAi4qktuw8yY0URM1Zu44uVRezYX4IZdG+eysAO6QzsmE7PVg01qtBRqMBFJGyUlTsWb9rF9BVFzFhRxIINOykrdyTHRTOgfRMGdkxnYId0WjbWWS2gAheRMLbrQAmzVm1jxsoiZqzYxqadBwBom5ZYUeYd0+jdqnG9vQWuClxEIoJzjtVFe5m+YhszVhTx9ZrvKS4tByoKPadVQ3q2bEjPVo3olJlcL3a5qMBFJCIdLClj/rodLNiwkwXrd5K/YQfb9h4CIC46iu7NU+nZqiE5LRvRs1VDslLj69xZLipwEakTnHNs3HGA/EqFvnjzbg75t9IzkuN+KPSclg3JbpFKYly0x6lrpqoCj+z/KhGpd8yMlo0b0LJxA87r0QyAQ6XlLC3c7S/1HeRv2MnUJVsAiDLo2DSZDk2TaZuWSLuMJNqlJ9I2LYmEWJ+X/yk1pi1wEamTtu87xMINO1mwYScFG3eyumgvG3ccoHLlNW+YQNv0RNqlJ/3oOTMlvHbDhGQL3MwGA48DPuB559z9Nfn3RESCpXFiLIM6ZzCoc8YP0w6WlLH2+32s3rqPNUV7WV20l9VF+3grbwP7DpX9MF9irI8T/IXeLj2JE9ISyUyNJy0pjvTkOBJjfWFR8NUucDPzAU8DZwMbgblm9r5z7ptghRMRCab4GB+dM1PonJnyo+nOObbsLv5Rqa8u2kve2h28l7/5J/9OQoyP9OSKMk9Liq14nRT/4/fJcaQlxREfE7rdNDXZAu8LrHLOrQEws9eB4YAKXEQiipmRmRpPZmo8A9qn/eiz/YdKWff9for2FFc89lY8b/M/rynax+xvt7Nzf8lR/+3k+GjSk+O478Lu9G/bJKi5a1LgzYENld5vBPodOZOZjQZGA7Rq1aoGPyciUvsaxEbTJSuFLlk/P9+h0nK+31f8Q9EfLvjDpZ+aEPyLkGpS4EfbAfSTI6LOubHAWKg4iFmD3xMRCVux0VFkpSaQlVp790GvySVMG4GWld63AH66s0hEREKiJgU+F+hgZieYWSxwGfB+cGKJiMixVHsXinOu1MxuBaZScRrheOfckqAlExGRn1Wj88Cdcx8CHwYpi4iIHIe6fxsvEZE6SgUuIhKhVOAiIhFKBS4iEqFq9W6EZlYErKvm19OAbUGME2zKVzPKVzPKV3PhnLG1cy79yIm1WuA1YWZ5R7udYrhQvppRvppRvpqLhIxH0i4UEZEIpQIXEYlQkVTgY70OcAzKVzPKVzPKV3ORkPFHImYfuIiI/FgkbYGLiEglKnARkQjleYGbWbyZzTGzhWa2xMzuPco8ZmZPmNkqMysws16VPhtsZsv9n93jUb4r/LkKzGyWmfWo9NlaM1tkZvlmludRvtPNbJc/Q76Z/bnSZ+Gw/H5bKdtiMyszs8b+z0K6/Cpl8JnZAjObcpTPPFv/Aszn2foXYD7P1r8A83m+/lWbc87TBxUj+yT5X8cAs4H+R8wzBPjIP29/YLZ/ug9YDbQFYoGFQFcP8g0AGvlfn3s4n//9WiDN4+V3OjDlKN8Ni+V3xPznAZ/W1vKr9Dt3AZOqWE6erX8B5vNs/Qswn2frXyD5wmH9q+7D8y1wV2Gv/22M/3HkkdXhwEv+eb8GGppZFpUGVnbOHQIOD6xcq/mcc7Occzv8b7+mYnSiWhHg8qtKWCy/I1wOvBbMDMdiZi2AocDzVczi2foXSD4v1z8IaPlVJSyW3xFqff2rCc8LHH748yYf2ApMc87NPmKWow2g3Pxnptd2vsquo2Jr7TAHfGxm86xigOegCzDfSf7dGB+ZWTf/tLBafmbWABgMTK40OeTLD3gM+B1QXsXnnq5/AeSrrNbXPwLL59n6F2A+L9e/aguLAnfOlTnncqjYcuhrZiceMUtVAygHNLByTQWQDwAzG0TF/4DurjT5ZOdcLyr+tL3FzAZ6kG8+FfdS6AE8CfzzcOSj/XMe5DvsPOBL59z2StNCuvzMbBiw1Tk37+dmO8q0Wln/Asx3eN5aX/8CzOfZ+nc8yw8P1r+aCosCP8w5txP4nIr/F6ysqgGUa3Vg5Z/Jh5llU/En2nDn3PeVvrPZ/7wVeJeKPxtrNZ9zbvfh3RiuYhSlGDNLI4yWn99lHPHnay0sv5OB881sLRV/wp9hZq8cMY+X618g+bxc/46Zz+P1L6Dl5+fF+lczXu+EB9KBhv7XCcBMYNgR8wzlxweR5vinRwNrgBP4/wdBunmQrxWwChhwxPREILnS61nAYA/yZfL/L9rqC6z3L8uwWH7+z1KB7UBibS6/IzKcztEPtnm2/gWYz7P1L8B8nq1/geQLl/WvOo8ajYkZJFnARDPzUfEXwZvOuSlmdhOAc24MFeNuDqFiJd0PXOP/rDYGVg4k35+BJsAzZgZQ6iruatYUeNc/LRqY5Jz7twf5LgJuNrNS4ABwmatYK8Nl+QFcCHzsnNtX6bu1sfyOKozWv0Dyebn+BZLPy/UvkHwQZutfoHQpvYhIhAqrfeAiIhI4FbiISIRSgYuIRCgVuIhIhFKBi4hEKBW4iEiEUoGLiESo/wdNozpZaj6CPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sweep_w = np.arange(3, 5, 0.1)\n",
    "result = np.array([compute_loss_with_weight(w) for w in sweep_w])\n",
    "plt.plot(sweep_w, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe74890",
   "metadata": {},
   "source": [
    "A few notes:\n",
    "\n",
    "* When optimizing, you're thinking about the loss with respect to the parameters in the network for a given example.  The example is a constant!\n",
    "* The slope of the loss at some point can change dramatically as you depart from that point. \n",
    "\n",
    "### \"Backprop\"\n",
    "\n",
    "The fact our gradient was the product of two terms, $\\frac{dL}{d\\hat{y}}$ and $\\frac{d\\hat{y}}{dw}$ is the source of the term backprop - each partial product from left to right corresponds with the gradient back from the loss to some point in the network.  This partial product of the gradient back to a particular point in the network is sometimes called the \"error signal\" to that layer.  In our simple example, it only gets one layer deep.\n",
    "\n",
    "![linearbp](linearbp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c15b7",
   "metadata": {},
   "source": [
    "## A more sophisticated example\n",
    "\n",
    "Let's make our network two layer with a relu in the middle:\n",
    "\n",
    "\n",
    "$L = (\\hat{y} - y)^2$\n",
    "\n",
    "$\\hat{y} = h_1 w_2$\n",
    "\n",
    "$h_1 = relu(z_1)$\n",
    "\n",
    "$z_1 = x w_1$\n",
    "\n",
    "![twolayer](twolayer.png)\n",
    "\n",
    "In this example, $\\theta$ consists of both $w_1$ and $w_2$.  As before, we compute the derivative of the loss with respect to each parameter.  As we'll see, the gradient with respect to $w_1$ with include a lot of the same terms as $w_2$ and $w_2$ itself is exactly the same as above.  This makes sense because from that point in the network onwards, it *is* the same, only with $h_1$ instead of x as the \"features\".\n",
    "\n",
    "$\\frac{dL}{dw_2}= \\frac{dL}{d\\hat{y}} \\times \\frac{d\\hat{y}}{dw_2}$\n",
    "\n",
    "$= (2\\hat{y} - 2y) \\times h_1$\n",
    "\n",
    "Computing the gradient with respect to $w_1$ is similar:\n",
    "\n",
    "$\\frac{dL}{dw_1} = \\frac{dL}{d\\hat{y}} \\times \\frac{d\\hat{y}}{dw_1}$\n",
    "\n",
    "$= (2\\hat{y} - 2y) \\times w_2\\frac{dh_1}{dw_1}$ (note, $\\frac{dw_2}{dw_1}$ = 0)\n",
    "\n",
    "$= (2\\hat{y} - 2y) \\times w_2(z_1 > 0)\\frac{z_1}{dw_1}$\n",
    "\n",
    "$= (2\\hat{y} - 2y) \\times w_2(z_1 > 0)x$\n",
    "\n",
    "That $(2\\hat{y} - 2y)$ is in both calculations shouldn't be a suprise - $\\frac{dL}{d\\hat{y}}$ is the first step of both!  This is the \"error signal\" being \"backpropagated\" to the layer below and is shared by all gradient calculations beyond that point in the network.  Similarly the subsequent $(2\\hat{y} - 2y)w_2(z_1 > 0)$ component is shared by any gradient calculations earlier than the second layer of the network.  This slow buildup of shared gradient calculations from the loss back towards the features (and the efficiency of computing these values once and reusing them) gives the calculation a \"backwards\" flow - the gradient computation propagates back from the loss.\n",
    "\n",
    "## What if you have multiple weights in a layer?\n",
    "\n",
    "Up until this point, we only had a single scalar weight in each layer.  If you have a vector of weights (as usual in linear regression) or a matrix of weights (in intermediate layers of deep learning), nothing changes.  You compute the gradient of the loss with respect to each scalar value in those vectors/matrices.  The key question you continue to try to answer is \"I'll hold every other weight in this network fixed and tweak just this single one - if I move it slightly higher, does the loss go up or down, and by how much?\".\n",
    "\n",
    "There is a set of [matrix calculus patterns](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) when doing this that can be employed to compute the derivative of the loss with respect to all the scalars in a particular matrix of parameters all at once (as you'd imagine most of the work is duplicative)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
