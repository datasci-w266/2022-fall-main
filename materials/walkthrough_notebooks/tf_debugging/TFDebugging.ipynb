{"cells":[{"cell_type":"markdown","metadata":{"id":"RLNB42DtYoLe"},"source":["# Tensorflow debugging basics\n","\n","This notebook describes basic strategies for debugging models written in tensorflow.  We use a buggy version of the basic text classification example and debug it.\n","\n","\n","Colab has very limited debugging support:  although we've been running our models in colab so far, In order to verify correctness of any model, it is useful to run the code locally and overfit on a small dataset. Therefore **run this notebook locally on your computer** within [VSCode](https://code.visualstudio.com/). if you run into memory issues, try using a smaller batch size.\n","\n","This notebook uses VSCode for illustration but alternatively, you should also be able to use Pycharm.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Zgw7hrSYoLh"},"source":["#### Prerequisite: Install VSCode and relevant plugins\n","\n","1. Follow [these instructions](https://code.visualstudio.com/docs/setup/setup-overview) to install VSCode. \n","2. Install VSCode Plugins for [Python](https://code.visualstudio.com/docs/python/python-tutorial) and [Jupyter](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).\n","3. Make sure you're using python version 3.9 or 3.10 by running the below\n","\n","```\n","python --version\n","```\n","\n","_Python 3.10.0_\n","\n","4. Setup a new virtual environment. On the command line (Powershell on windows, Terminal on Mac) run the following commands one at a time to create a new virtual env. In the example below, `debug_notebook` is our working directory and `tfdebug` is the name of the virtual env. You can pick any names you like.\n","\n","```\n","mkdir debug_notebook\n","cd debug_notebook\n","python -m pip install --upgrade pip\n","python -m venv tfdebug\n","```\n","`source tfdebug/bin/activate` [`.\\tfdebug\\Scripts\\activate` on windows]\n","\n","\n","5. Install required libraries in virtualenv `tfdebug` by running following commands on the command line\n","\n"," ```\n","pip install ipykernel sklearn nltk matplotlib\n","pip install tensorflow\n","pip install tensorflow-datasets\n","pip install pydot\n","pip install transformers\n"," ```\n","\n"," 6. Copy this notebook into the `debug_notebook` directory created \n"," 7. Below command launches vscode. Open this notebook within the IDE\n","```\n","code .\n","\n","```\n","8. Ensure that the python version used within vscode points to the version within the `tfdebug` venv as seen in image below\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/html":["<img src=\"http://drive.google.com/uc?export=view&id=1zKmOCFOjbLMbv8tW4v8a-gF0l3iIA1kc\">\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","%%html\n","<img src=\"http://drive.google.com/uc?export=view&id=1zKmOCFOjbLMbv8tW4v8a-gF0l3iIA1kc\">"]},{"cell_type":"markdown","metadata":{},"source":["##### Turn off existing GPU if any"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1v8fZwHSYoLh"},"outputs":[],"source":["os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"]},{"cell_type":"markdown","metadata":{"id":"5tAC3GO1YoLi"},"source":["#### Step debugging\n","\n","(You can skip this section if you're familiar with debugging within an IDE)\n","\n","If you have the required setup, you should be able to step through this _(poorly written)_ fibonacci sequence below within VSCode. Try to find the bug by stepping through this block. \n","\n","1. Add a breakpoint within the `for` loop\n","2. Instead of running this block below, click on the dropdown next to the run icon in cell below and click debug instead.\n","3. Step through the code using the icons at the top. Make sure you understand stepping over, stepping into, step out and conditional evaluation\n","4. Try evaluating an expression in the debug console (screenshot below)\n","\n","Check the [VSCode documentation](https://code.visualstudio.com/docs/editor/debugging) for more details.\n","\n","_hint: when does the function terminate_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fib(n):\n","    return fib(n - 1) + fib(n - 2) \n","\n","print(fib(5))"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<img src=\"http://drive.google.com/uc?export=view&id=15r86iTVU9QG-bSNDyh5RzqBch7DjDevR\">\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","%%html\n","<img src=\"http://drive.google.com/uc?export=view&id=15r86iTVU9QG-bSNDyh5RzqBch7DjDevR\">"]},{"cell_type":"markdown","metadata":{"id":"qM3AsqKsYoLj"},"source":["### Debugging BERT Classification model\n","\n","In order to demonstrate debugging a tensorflow model, we use the BERT Classification Model example from the lesson 4 notebook. Assuming that resources are limited on local computers, we load 20 records for training and 5 for test. The setup code has been hidden so we can focus on the debugging concepts. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfXsSVh0YoLj"},"outputs":[],"source":["#@title\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.backend as K\n","import tensorflow_datasets as tfds\n","\n","\n","\n","import sklearn as sk\n","import os\n","import nltk\n","from nltk.data import find\n","\n","import matplotlib.pyplot as plt\n","\n","import re\n","\n","\n","from transformers import BertTokenizer, TFBertModel\n","\n","train_data, test_data = tfds.load(\n","    name=\"imdb_reviews\", \n","    split=('train[:80%]', 'test[80%:]'),\n","    as_supervised=True)\n","\n","num_train_examples = 20      # set number of train examples - 1500 for realtime demo\n","num_test_examples = 5        # set number of test examples - 500 for realtime demo\n","\n","#make it easier to use a variety of BERT subword models\n","model_checkpoint = 'bert-base-cased'\n","\n","train_examples, train_labels = next(iter(train_data.batch(num_train_examples))) # load 2000 records for training\n","test_examples, test_labels = next(iter(test_data.batch(num_test_examples))) # load 500 records for test\n","\n","# BERT Tokenization of training and test data\n","max_length = 128                 # set max_length\n","\n","all_train_examples = [x.decode('utf-8') for x in train_examples.numpy()]\n","all_test_examples = [x.decode('utf-8') for x in test_examples.numpy()]\n","\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","\n","x_train = bert_tokenizer(all_train_examples[:num_train_examples],\n","              max_length=max_length,\n","              truncation=True,\n","              padding='max_length', \n","              return_tensors='tf')\n","y_train = train_labels[:num_train_examples]\n","\n","x_test = bert_tokenizer(all_test_examples[:num_test_examples],\n","              max_length=max_length,\n","              truncation=True,\n","              padding='max_length', \n","              return_tensors='tf')\n","y_test = test_labels[:num_test_examples]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZDFljk--YoLk"},"source":["#### Custom model implementation and eager execution\n","\n","In previous examples and notebooks in the course so far, we've been training our models using high level APIs, such as `model.fit`, which hides most of the complexity. However, Keras allows you to [customize](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) what happens in `model.fit`, exposing the underlying steps. This can be useful for more complex models as well as debugging. Keras provides *progressive disclosure of complexity* when needed, therefore, the `train_step` and `test_step` functions can be removed and the model will still work using the default implementation in the base class. \n","\n","It is also important to understand the difference between the default Graph mode vs the eager mode. [This link](https://www.tensorflow.org/guide/intro_to_graphs) has a good explanation of the difference. We need to run in eager mode in order to debug the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpP-OKCSYoLl"},"outputs":[],"source":["from inspect import trace\n","from numpy import int32\n","import traceback\n","import pickle\n","\n","class BertClassificationModel(Model):\n","    def __init__(self,  checkpoint, hidden_size=201,  dropout=0.3):\n","        super().__init__()\n","        self.bert_model = TFBertModel.from_pretrained(checkpoint)\n","        self.top_layer = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')\n","        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='classification_layer')\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","        self.hidden_size = hidden_size\n","\n","    def call(self, inputs, training=True):\n","        input_ids, token_type_ids, attention_mask = inputs\n","\n","        result = self.bert_model(input_ids, token_type_ids, attention_mask)\n","        cls_out = result[1]\n","        hidden = self.top_layer(cls_out)\n","        hidden = self.dropout(hidden)\n","        classification = self.classifier(cls_out)\n","        return classification\n","\n","    def train_step(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        x, y = data\n","        self.x, self.y = x, y\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute the loss value\n","            # (the loss function is configured in `compile()`)\n","            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        # Update weights\n","        self.optimizer.apply_gradients((grad, var) for grad, var in zip(gradients, trainable_vars) if grad is not None)\n","        # Update metrics (includes the metric that tracks the loss)\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","        self.x, self.y = x, y\n","\n","        # Compute predictions\n","        y_pred = self(x, training=False)\n","        # Updates the metrics tracking the loss\n","        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {m.name: m.result() for m in self.metrics}"]},{"cell_type":"markdown","metadata":{"id":"4gxBYjThYoLl"},"source":["#### Debug the code block below\n","\n","The block below does the same thing as the one in the lesson notebook, however has an error. In order to debug, set the `run_eagerly` flag to `true`. This will prevent the model from executing in graph mode and we're able to step through the model execution.\n","Step through the code and check where the code fails. Try different things:\n","* Add a breakpoint somewhere within the `train_step` function\n","* Step through the code\n","* Check the shapes of the `x` and `y`. See the values of `x` and `y`\n","* Evaluate the output of the `self.compiled_loss(..)` call directly in the debug console\n","\n","Once the bug is fixed, set `run_eagerly` to `False` and then try to debug. Does it work?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtqJA4A1YoLl"},"outputs":[],"source":["bert_classification_model = BertClassificationModel(checkpoint=model_checkpoint)\n","\n","bert_classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n","                                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                                metrics='accuracy')\n","bert_classification_model.run_eagerly = True\n","try:\n","    bert_classification_model_history = bert_classification_model.fit(\n","        [x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n","        y_train,\n","        validation_data=([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask], y_test),\n","        batch_size=2,\n","        epochs=2\n","    )\n","except Exception as e:\n","    bert_classification_model.save_weights('cl_model_weights.ckpt')\n","    features_dict = {}\n","    for n, v in enumerate(bert_classification_model.x):\n","        features_dict[n] = v.numpy()\n","    with open('cl_features.pkl','wb') as f:\n","        pickle.dump(features_dict,f)\n","    labels_dict = {}\n","    labels_dict[\"y\"] = bert_classification_model.y\n","    with open('cl_labels.pkl', 'wb') as f:\n","        pickle.dump(labels_dict, f)\n","    raise e"]},{"cell_type":"markdown","metadata":{"id":"1vrlErjzYoLm"},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('tfenv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9fd3981ab98582727503927be8cd5f80bc644b36484304487fb87af38a262e15"}}},"nbformat":4,"nbformat_minor":0}
