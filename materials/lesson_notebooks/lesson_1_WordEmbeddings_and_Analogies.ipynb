{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tJxS2yLtMiq"
   },
   "source": [
    "## Experiment with Embeddings\n",
    "\n",
    "Note: the lesson notebooks are created for the purpose of in-class illustrations and student experimentation after class.\n",
    "\n",
    "Back to [week 1 slides](https://docs.google.com/presentation/d/1nHIybSe1OefjxJkdhOJg43by-Sk1k8_wYoBw6Uj50NI/edit#slide=id.g1267b8de356_1_6)\n",
    "\n",
    "How should we think about embeddings relative to language?  How do they represent words? Are they like dictionary definitions of words with clear boundaries?  Are they a sharp clear respresentation of the meaning or are they more nebulous?\n",
    "\n",
    "Gensim is a library that is used for working with similarity of words and documents.  We'll use it to experiment with embeddings and to sharpen our intuition.  Make sure you use the correct version of gensim for this notebook.\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2022-fall-main/blob/master/materials/lesson_notebooks/lesson_1_WordEmbeddings_and_Analogies.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wRy3EBdto4A",
    "outputId": "6d61ff08-753f-4294-c3e0-a9cc9573169e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/mhbutler/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mhbutler/anaconda3/lib/python3.9/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/mhbutler/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/mhbutler/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CS0Y5T3etDIA"
   },
   "outputs": [],
   "source": [
    "# we'll use some other libraries as well\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKecg4abtPCn"
   },
   "source": [
    "Gensim has a preloaded set of different well known static word embeddings.  We'll take advantage of these to explore how they represent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1xgv5LjtPso",
    "outputId": "3f094245-6b31-4baf-95a1-c29d34f106ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YiXMjT0vENT"
   },
   "source": [
    "### Fits like a GloVe?\n",
    "\n",
    "Word embeddings take a long time to train - since the goal is to provide a good representation for as many words as possible, generating good embeddings often requires making several passes over a very large corpus. There are a number of different set out there avaialble for download.\n",
    "\n",
    "Fortunately, it's possible to learn fairly general embeddings from large corpora that are useful for many downstream tasks. We'll use the GloVe vectors available at https://nlp.stanford.edu/projects/glove/ - specifically, a set trained with a vocabulary of 400,000 on a corpus of 6B tokens from Wikipedia and Gigaword.\n",
    "\n",
    "The vectors are distributed as a (very) large text file, with one word per line followed by its vector:\n",
    "```\n",
    "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\n",
    "```\n",
    "\n",
    "Gensim makes a number of these static embeddings avialble for your use.  They provide access to fasttext, conceptnet, word2vec, and GloVe.\n",
    "\n",
    "You can select the set you want to use by parsing the name of the models provided by gensim.  These embeddings are identified by several dimensions.  First is the language modeling task that was used to build the embeddings.  Second is the corpus used to generate the embeddings.  Last is the dimensionality (size) of each embedding. The file 'glove-wiki-gigaword-50' means the glove approach was used to create the embeddings.  wiki-gigaword is the corpus used for training.  These embeddings are 50 dimensions long. \n",
    "\n",
    "\n",
    "\n",
    "If you download word2vec it takes a long time (up to 15 minutes).  The glove-wiki-gigaword-50 takes significantly less time to download(around 1 minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6Sz34TTvF_T",
    "outputId": "70719138-46b8-4ad2-bff1-138335d34860"
   },
   "outputs": [],
   "source": [
    "# Download the original \"word2vec-google-news-300\" embeddings\n",
    "\n",
    "embed_vectors = gensim.downloader.load('glove-wiki-gigaword-50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings consist of a set of vectors and each vector is associated with a word.  This makes it easy to look up the vector for a specific word.  The list of words for which there are vectors are held in a vocabulary list.  Let's get the list and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many words in our vocabulary?\n",
    "embed_vectors_vocab = embed_vectors.index_to_key\n",
    "len(embed_vectors_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the words in the vocabulary.  We can retrieve them by position in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chanty', 'kronik', 'rolonda', 'zsombor', 'sandberger']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors_vocab[399995:400000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one vector and see what it contains.  You can try various words to see what words are in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8932  , -0.74191 ,  0.74033 , -1.2181  ,  0.38596 ,  0.22143 ,\n",
       "        0.87075 ,  0.089573, -0.258   ,  0.87708 , -0.22681 ,  0.45953 ,\n",
       "        1.321   ,  1.132   , -0.72951 ,  0.92633 ,  1.2474  , -0.5903  ,\n",
       "        0.23094 ,  0.017891,  0.023733, -1.1873  ,  1.8662  ,  0.18656 ,\n",
       "       -0.63184 ,  0.95206 , -0.55096 ,  1.0844  , -0.43715 ,  0.11234 ,\n",
       "       -0.38582 , -0.36061 ,  0.63165 ,  0.57202 ,  0.38001 ,  0.081306,\n",
       "       -0.41507 ,  0.015277,  0.79508 , -0.24794 , -0.24346 ,  0.65275 ,\n",
       "       -0.69327 ,  0.37766 ,  0.13726 ,  0.18225 , -0.05881 , -0.18726 ,\n",
       "        0.37905 , -0.74832 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec0 = embed_vectors['globules']\n",
    "\n",
    "vec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.9149e-01,  8.6617e-01,  1.1998e-01,  9.2287e-04,  2.7760e-01,\n",
       "       -4.9185e-01,  5.0195e-01,  6.0792e-04, -2.5845e-01,  1.7865e-01,\n",
       "        2.5350e-01,  7.6572e-01,  5.0664e-01,  4.0250e-01, -2.1388e-03,\n",
       "       -2.8397e-01, -5.0324e-01,  3.0449e-01,  5.1779e-01,  1.5090e-02,\n",
       "       -3.5031e-01, -1.1278e+00,  3.3253e-01, -3.5250e-01,  4.1326e-02,\n",
       "        1.0863e+00,  3.3910e-02,  3.3564e-01,  4.9745e-01, -7.0131e-02,\n",
       "       -1.2192e+00, -4.8512e-01, -3.8512e-02, -1.3554e-01, -1.6380e-01,\n",
       "        5.2321e-01, -3.1318e-01, -1.6550e-01,  1.1909e-01, -1.5115e-01,\n",
       "       -1.5621e-01, -6.2655e-01, -6.2336e-01, -4.2150e-01,  4.1873e-01,\n",
       "       -9.2472e-01,  1.1049e+00, -2.9996e-01, -6.3003e-03,  3.9540e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec0 = embed_vectors['unk']\n",
    "\n",
    "vec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.079084, -0.81504 ,  1.7901  ,  0.91653 ,  0.10797 , -0.55628 ,\n",
       "       -0.84427 , -1.4951  ,  0.13418 ,  0.63627 ,  0.35146 ,  0.25813 ,\n",
       "       -0.55029 ,  0.51056 ,  0.37409 ,  0.12092 , -1.6166  ,  0.83653 ,\n",
       "        0.14202 , -0.52348 ,  0.73453 ,  0.12207 , -0.49079 ,  0.32533 ,\n",
       "        0.45306 , -1.585   , -0.63848 , -1.0053  ,  0.10454 , -0.42984 ,\n",
       "        3.181   , -0.62187 ,  0.16819 , -1.0139  ,  0.064058,  0.57844 ,\n",
       "       -0.4556  ,  0.73783 ,  0.37203 , -0.57722 ,  0.66441 ,  0.055129,\n",
       "        0.037891,  1.3275  ,  0.30991 ,  0.50697 ,  1.2357  ,  0.1274  ,\n",
       "       -0.11434 ,  0.20709 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = embed_vectors['computer']  # get numpy vector of a word\n",
    "\n",
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = embed_vectors['memory']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "To measure the similarity of two words, we'll use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between their representation vectors:\n",
    "\n",
    "$$ D^{cos}_{ij} = \\frac{v_i^T v_j}{||v_i||\\ ||v_j||}$$\n",
    "\n",
    "*Note that this is called cosine similarity because $D^{cos}_{ij} = \\cos(\\theta_{ij})$, where $\\theta_{ij}$ is the angle between the two vectors.*\n",
    "\n",
    "\n",
    "Let's use numpy to calculate the cosine similarity between two vectors.  The closer the distance between the two vectors the more alike they are.  In word2vec the embeddings are built so that words that are used in the same context have more similar vectors than words used in different contexts.  If you think of names of cities like London or Paris they will be used in the same context like \"I want to visit ...\" or \"I flew in from ...\" or \"I once lived in ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6909388"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = np.dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6909387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.similarity(w1='computer', w2='memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a word like 'flies' which is ambiguous.  As a noun it refers to an insect.  As a verb it refers to the act of flying.  Notice how both of these are incorporated into the embedding and how this is reflected in the list of most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RoL25Ic5mKd",
    "outputId": "9fff0b1b-b012-46f2-9bdd-d75746944244"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fly', 0.7638436555862427),\n",
       " ('flying', 0.7358503937721252),\n",
       " ('birds', 0.6993444561958313),\n",
       " ('moths', 0.6920357942581177),\n",
       " ('butterflies', 0.6906600594520569),\n",
       " ('plane', 0.6849700808525085),\n",
       " ('circling', 0.6725693941116333),\n",
       " ('spotted', 0.672272801399231),\n",
       " ('sea', 0.6711998581886292),\n",
       " ('tsetse', 0.6690365672111511)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim allows us to easily see which words are most similar to the word we specify.\n",
    "embed_vectors.most_similar('flies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmtADXzEzTIY"
   },
   "source": [
    "The results are not always what you might expect.  It is useful to think about what you expect to see and what associations are actually represented and why.  For example we think of happy as the emotion.  Notice that most similar doesn't necessarily mean sysnonmys.  It just means used often in the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztILB5BnzUH1",
    "outputId": "9b309250-5492-4f78-d040-caa8fde8363a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'m\", 0.9142324328422546),\n",
       " ('everyone', 0.8976402282714844),\n",
       " ('everybody', 0.8965491056442261),\n",
       " ('really', 0.8839760422706604),\n",
       " ('me', 0.878463089466095),\n",
       " ('definitely', 0.8762789368629456),\n",
       " ('maybe', 0.8756702542304993),\n",
       " (\"'d\", 0.8718011975288391),\n",
       " ('feel', 0.8707677721977234),\n",
       " ('i', 0.8707453012466431)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzXCYyfw1qx7",
    "outputId": "8b689ed0-ba5d-4f99-8af4-eab068f2ce6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('then-director', 0.693364143371582),\n",
       " ('carneades', 0.6816099286079407),\n",
       " ('bb94', 0.6682854294776917),\n",
       " ('vopi', 0.6628191471099854),\n",
       " ('endel', 0.6545295119285583),\n",
       " ('dovers', 0.6493183374404907),\n",
       " ('cw96', 0.6464378237724304),\n",
       " ('synesius', 0.6453444957733154),\n",
       " ('kd94', 0.6443257927894592),\n",
       " ('25aou94', 0.6437767744064331)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see least similar use the following construct\n",
    "embed_vectors.most_similar(negative=['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8ME-Zf6zzt0"
   },
   "source": [
    "### Analogies\n",
    "\n",
    "We can also use these embeddings to perform analogies.  This is an interesting way to test how well they represent the meaning of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjg85h5fz0ZO",
    "outputId": "fc0db09d-9a84-4d36-c232-446bb7dd3d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.8524\n"
     ]
    }
   ],
   "source": [
    "result = embed_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGzaaP960k2_"
   },
   "source": [
    "Let's define a function and try some simple analogies.  When it works well it is great and when it fails it can be equally sepctacular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8Sg9qEth0lmJ"
   },
   "outputs": [],
   "source": [
    "def make_analogy(x1, x2, y1):\n",
    "    result = embed_vectors.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pPCnKrb19c-x",
    "outputId": "80d5f5ca-1f3b-4658-d8d7-3a634401f6e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastest'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_analogy('long', 'longest', 'fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-v1GtKAv9cwk",
    "outputId": "d5ffb2c3-4f45-4eaf-e153-87561d5b8466"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'length'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_analogy('hands', 'arms', 'feet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KY_qTDTf9chl",
    "outputId": "c6e0603e-0386-409b-a0ff-fba213cd52a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greece'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_analogy('paris', 'france', 'athens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MTfkypYC9b_7",
    "outputId": "26158812-6c3b-4346-b879-eb34933ff6b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prodigy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_analogy('man', 'programmer', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity and Language\n",
    "\n",
    "How does the similarity score translate to your understanding of language?  It is also good to experiment with words that you think are similar and words you think are different to see what kind of similarity scores you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tWWg-esZ7Ca",
    "outputId": "b53cdaab-6168-42a1-c9c1-a33cb409c047"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.similarity(w1='man', w2='guy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eP9plrEKa-rc",
    "outputId": "36dcce7b-d40c-449b-a193-364d60230346"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85644317"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.similarity(w1='man', w2='boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "898yvCMEbHSv",
    "outputId": "c333e5a1-b5ab-4dd7-f199-58e3b5b3effb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30944517"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.similarity(w1='man', w2='squirrel')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "WordEmbeddings and Analogies.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
